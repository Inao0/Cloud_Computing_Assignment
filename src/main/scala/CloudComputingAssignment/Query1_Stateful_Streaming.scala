package CloudComputingAssignment

import java.io.{File, FileWriter}
import java.util.Calendar

import org.apache.commons.io.FileUtils
import org.apache.spark.sql.functions._
import org.apache.spark.sql.streaming.{GroupState, GroupStateTimeout, OutputMode}
import org.apache.spark.sql.{ForeachWriter, SparkSession}

/** Provides a solution for parsing DEBS 2015 csv data files by using stateful streaming
 *
 * This implementation handles streaming data
 * This solution output one row per update and per time window with entire top included
 *
 * example output:
 * TimeWindow -								 Update-time :					[Route:Count,latest_dropoff]				[Route:Count,latest_dropoff],...
 *(2012-12-31 23:41:00.0,2013-01-01 00:11:00.0)-2020-02-09 05:08:53.821 : 	[((1,1),(1,300)) : 3,2013-01-01 00:10:30.0]	[((1,1),(300,1)) : 3,2013-01-01 00:10:00.0]	[((-1,-1),(-1,-1)) : -1,1970-01-01 01:00:00.0]
 *
 * The default output folder is /Result/Stateful-Streaming
 */
object Query1_Stateful_Streaming {

  /**
   * Folder for output files
   */
  val OUTPUT_FOLDER: String = RESULT_FOLDER + "/Stateful-Streaming"



  /** writerForText allows us to write the results from the query into a text file
   *
   * This writer process the optionnal state corresponding to a window and compute the corresponding top n if possible
   * It is then happened to a text file according to the spark partition id
   * The output results is split between different files in OUPUT_FOLDER according to spark partitioning
   *
   * This implementation is inspired by :
   * https://stackoverflow.com/questions/49026429/how-to-use-update-output-mode-with-fileformat-format
   *
   * ===Why can't we use a predefine output sink ?===
   *
   * According to ForeachWriter documentation this is "A class to consume data generated by a StreamingQuery"
   * Here we need to define one manually because :
   * - Queries with mapGroupsWithState support only Update output mode
   * - File Sink support only	Append mode
   *
   * Therefore predefined writers for files won't work :
   *
   * We therefore decide to use a Foreach Sink and the define the following ForeachWriter.
   *
   * This type of sink is said, according to http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks
   * to support update mode.
   * However when running the application as it is we obtain he following error message :
   *
   * " Caused by: java.lang.IllegalArgumentException: Data source v2 streaming sinks does not support Update mode.
   * at org.apache.spark.sql.execution.streaming.StreamExecution.createStreamingWrite(StreamExecution.scala:635)"
   *
   * By having a look at spark source code we find where the error is coming from (StreamExecution.scala:628) :
   * " case Update =>
   * // Although no v2 sinks really support Update mode now, but during tests we do want them
   * // to pretend to support Update mode, and  treat Update mode same as Append mode."
   * if (Utils.isTesting) {
   *           writeBuilder.buildForStreaming()
   * } else {
   * throw new IllegalArgumentException(
   * "Data source v2 streaming sinks does not support Update mode.")
   * }
   *
   * Therefore in order to avoid this else loop that throw this error no mater which type of output sink we use,
   * we export SPARK_TESTING=$true into the run-time environment to enter the if branch which seems the only way to
   * get an update mode in the current version of spark
   */
  val writerForText: ForeachWriter[Option[WindowStateOfTaxiRoutes]] = new ForeachWriter[Option[WindowStateOfTaxiRoutes]] {
    var fileWriter: FileWriter = _

    /** Open the file to be used as the output sink by the partition in the OUTPUT_FOLDER
     *
     * @param partitionId : spark partition id
     * @param version     : version number to handle the update mode properly
     */
    def open(partitionId: Long, version: Long): Boolean = {
      println("open : partition" + partitionId)
      FileUtils.forceMkdir(new File(OUTPUT_FOLDER))
      //we use true to append the file instead of erasing it each time we try to add newer values
      fileWriter = new FileWriter(new File( OUTPUT_FOLDER + s"/results_partition_$partitionId"), true)
      fileWriter.append("TimeWindow -\t\t\t\t\t\t\t\t Update-time :\t\t\t\t\t[Route:Count,latest_dropoff]\t\t\t\t[Route:Count,latest_dropoff],...   \n")
      true
    }

    /** Close the file used by the partition
     *
     */
    def close(errorOrNull: Throwable): Unit = {
      fileWriter.close()
    }

    /** Computes and outputs the new top N into the file if the state is not empty
     *
     * This is the function that defined the formatting of the output
     * Each WindowTopN is outputted following this example :
     * (1970-01-01 01:16:00.0,1970-01-01 01:46:00.0)-2020-02-08 22:51:41.737 : 	[((1,1),(300,1)) : 1,1970-01-01 01:16:40.0]	[((-1,-1),(-1,-1)) : -1,1970-01-01 01:00:00.0]	[((-1,-1),(-1,-1)) : -1,1970-01-01 01:00:00.0] ...
     *
     * which corresponds to
     * * TimeWindow - Update-time :  [Route:Count,latest_dropoff]  [Route:Count,latest_dropoff]  ...
     *
     * @param state : optionnal state corresponding to a window and each route count and last drop-off associated
     */
    def process(state: Option[WindowStateOfTaxiRoutes]): Unit = {
      if (state.isDefined) {
        val value = getTopN(state.get, TOP_SIZE)
        println("process " + value.time_window.toString() + value.currentTop.mkString(";")) //give a visual feedback in the consol of processed windows
        fileWriter.append(value.time_window.toString() + "-" + value.last_update + " : \t")
        for (ride <- value.currentTop) {
          fileWriter.append("[" + ride.route.toString() + " : " + ride.countAndLatestDropoff.count + "," + ride.countAndLatestDropoff.latest_dropoff + "]\t")
        }
        fileWriter.append("\n")
      }
    }
  }


  /** Returns the n most frequented Routes for a time window according to the provided windowStateOfTaxiRoutes
   *
   * The ranking of the different routes relies on the ordered trait implemented by CountAndLatestDropOff case class.
   *
   * If there is not enough routes in windowStateOfTaxiRoutes the remaining spot in the top n are filled with
   * the following RidesTopNElement : (((-1,-1),(-1,-1)),-1,1970-01-01 01:00:00.0)
   *
   * The returned windowTopN last_update is filled with a timestamp corresponding to the current date and time
   *
   * @param windowStateOfTaxiRoutes WindowStateOfTaxiRoutes summarising the last count and latest drop-off time for each routes in a time window
   * @param n                       Number of elements in the final ranking top
   * @return WindowTopN containing the n most frequented routes for the time window
   */
  def getTopN(windowStateOfTaxiRoutes: WindowStateOfTaxiRoutes, n: Int): WindowTopN = {

    val result = new Array[RoutesTopNElements](n)

    /* we need to get the n most frequented routes for the time window. We use the ordering relationship define on
    CountAndLatestDropoff to get the max we use a mutable copy of the WindowStateOfTaxiRoutes currentState (as it is a map
    containing CountAndLatestDropoff indexed by routes). We then get the max value from the map and remove it from the
     collection. We repeat this to get the n most frequented routes according to the windowStateOfTaxiRoutes*/
    val values = collection.mutable.Map[Route, CountAndLatestDropoff](windowStateOfTaxiRoutes.currentState.toSeq: _*) //Potential memory issue ?

    for (i <- 0 until n) {
      if (values.nonEmpty) {
        val maxValue = values.maxBy(_._2)
        /*We create a RoutesTopNElements from the maxValue index in the map (._1) which is a Route a and the CountAndLatestDropoff value*/
        result(i) = RoutesTopNElements(maxValue._1, maxValue._2)
        /*Removing the max from the map*/
        values.-=(maxValue._1)
      } else {
        /*If there is not enough routes in windowStateOfTaxiRoutes we fill the remaining spot in the top n with
        the following RidesTopNElement : (((-1,-1),(-1,-1)),-1,1970-01-01 01:00:00.0)
        */
        result(i) = RoutesTopNElements(((-1, -1), (-1, -1)), CountAndLatestDropoff(-1, new java.sql.Timestamp(0L)))
      }
    }

    /* current time is used to signify when this top has been updated*/
    val now: java.util.Date = Calendar.getInstance.getTime
    WindowTopN(windowStateOfTaxiRoutes.time_window, result, java.sql.Timestamp.from(now.toInstant))
  }

  /** Processes new taxi Rides corresponding to the window and keeps track of the previous taxi Rides and maps each
   * timeWindow to an optionnal state containing the updated information
   *
   * @param timeWindow timeWindow
   * @param values     processed taxi rides corresponding to the window
   * @param state      A WindowStateOfTaxiRoutes state object set by previous invocations of the given function and corresponding
   *                   to this key window. It contains the list of all the processed taxi rides corresponding to this time window
   * @return optionnal state corresponding to the window
   */
  def windowTopNMapping(timeWindow: MyTimeWindow, values: Iterator[NYCTaxiRide], state: GroupState[WindowStateOfTaxiRoutes]):
  Option[WindowStateOfTaxiRoutes] = {
    if (state.hasTimedOut) {
      //If the state has time out we want to remove it to free the corresponding memory
      state.remove()

      state.getOption
    } else {
      // get current state or create a new one if there's no previous state
      val currentState = state.getOption.getOrElse(WindowStateOfTaxiRoutes(timeWindow, Map[Route, CountAndLatestDropoff]()))
      // enrich the state with the new events. Potential memory issue ?
      val updatedCount = collection.mutable.Map[Route, CountAndLatestDropoff](currentState.currentState.toSeq: _*)
      values.foreach(ride => {
        // get the current count and last drop-off for the current ride route. If it doesn't exist set it to (0,1970-01-01 01:00:00.0)
        val current = updatedCount.getOrElse((ride.pickup_gridCell, ride.dropoff_gridCell), CountAndLatestDropoff(0, new java.sql.Timestamp(0L)))
        // add 1 to the current count for the route and update the latest drop-off to the most recent one between the current value and the value of the ride
        updatedCount((ride.pickup_gridCell, ride.dropoff_gridCell)) = CountAndLatestDropoff(current.count + 1, if (ride.dropoff_time.after(current.latest_dropoff)) ride.dropoff_time else current.latest_dropoff)
      })

      // update the state
      val updatedState = WindowStateOfTaxiRoutes(timeWindow, collection.immutable.Map(updatedCount.toList: _*))
      state.update(updatedState)


      state.getOption
    }
  }

  /** Main program - process DEBS 2015  NYC Taxi rides datafiles to solve a problem inspired by the first query of the challenge
   *
   * Loop that is executed when the class name is passed as argument alongside to the jar file to the spark-submit script
   * See README.md for more information about how to run this application
   *
   */
  def main(args: Array[String]) {

    /*Initialising spark context*/
    val spark = SparkSession.builder()
      .appName("CloudComputingAssignment-Sateful-Streaming")
      .getOrCreate()
    spark.sparkContext.setLogLevel("ERROR")
    //the following line can be commented if spark.sql.shuffle.partitions is to be set when calling spark submit
    //spark.sqlContext.sql("set spark.sql.shuffle.partitions=200") //should be adjusted according to the cluster

    /*Import implicits_ allows sparks to handle lots of the conversions automatically.
    * see sparks Encoders for more information */
    import spark.sqlContext.implicits._


    /*Parse the input files as a stream according to rawTaxiRidesStructType*/
    val InputTaxiRidesDf = spark.readStream.format("csv")
      .option("timeStampFormat", "yyyy-MM-dd HH:mm:ss")
      .schema(rawTaxiRidesStructType)
      .load(INPUT_FOLDER)
    InputTaxiRidesDf.printSchema()


    val taxiRidesWithSlidingWindow = InputTaxiRidesDf
      /*Remove unnecessary columns*/
      .select("dropoff_datetime", "pickup_longitude", "pickup_latitude", "dropoff_longitude", "dropoff_latitude")
      /*Add a sliding time window */
      .withColumn("time_window", window(col("dropoff_datetime"), WINDOW, SLIDE))

    taxiRidesWithSlidingWindow.printSchema()

    val NYCTaxiRidesRefinedDf = taxiRidesWithSlidingWindow
      /*Map reach row to a NYCTaxiRide */
      .map(row => NYCTaxiRide(
        (row.getStruct(5).get(0).asInstanceOf[java.sql.Timestamp], row.getStruct(5).get(1).asInstanceOf[java.sql.Timestamp]), //.asInstanceOf[MyTimeWindow]
        gridCellOf(row.getDouble(1), row.getDouble(2)),
        gridCellOf(row.getDouble(3), row.getDouble(4)),
        row.get(0).asInstanceOf[java.sql.Timestamp]))
      /*Filter out outliers rides having pickups are drop-off areas outside the 300x300 grid*/
      .filter($"pickup_gridCell._1" > 0 && $"pickup_gridCell._1" < 301 && $"pickup_gridCell._2" > 0 && $"pickup_gridCell._2" < 301)
      .filter($"dropoff_gridCell._1" > 0 && $"dropoff_gridCell._1" < 301 && $"dropoff_gridCell._2" > 0 && $"dropoff_gridCell._2" < 301)

    NYCTaxiRidesRefinedDf.printSchema()

    //Group by time window and map to optional states keeping track of corresponding routes counts and latest_drop-off
    val optionnalStatesByWindow = NYCTaxiRidesRefinedDf
      .withWatermark("dropoff_time", DELAY)
      .as[NYCTaxiRide]
      .groupByKey(rides => rides.time_window).mapGroupsWithState(GroupStateTimeout.EventTimeTimeout)(windowTopNMapping)


    //Set up output with the writerForText that process the optionnal states for each window and launch the Streaming Query
    optionnalStatesByWindow
      //.coalesce(3) can be uncommented to limit the number of spark partitions and less output files
      .writeStream
      .outputMode(OutputMode.Update())
      .foreach(writerForText)
      //.option("checkpointLocation", "./checkpoint") //should be uncommented to implement checkpoints
      .start()
      .awaitTermination()

  }

}

